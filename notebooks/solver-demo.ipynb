{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750474ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train shallow neural networks on a synthetic classification dataset using convex optimization.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6284d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scnn.private.utils.data import gen_classification_data\n",
    "\n",
    "\n",
    "from scnn.models import ConvexGatedReLU, ConvexReLU\n",
    "from scnn.solvers import RFISTA, AL, LeastSquaresSolver, CVXPYSolver, ApproximateConeDecomposition\n",
    "from scnn.regularizers import NeuronGL1, L2, L1\n",
    "from scnn.metrics import Metrics\n",
    "from scnn.activations import sample_gate_vectors\n",
    "from scnn.optimize import optimize_model, optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf47351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate realizable synthetic classification problem (ie. Figure 1)\n",
    "n_train = 1000\n",
    "n_test = 1000\n",
    "d = 50\n",
    "hidden_units = 100\n",
    "kappa = 10  # condition number\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = gen_classification_data(123, n_train, n_test, d, hidden_units, kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd5ec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 0.001\n",
    "max_neurons = 500\n",
    "G = sample_gate_vectors(123, d, max_neurons)\n",
    "metrics = Metrics(metric_freq=25, model_loss=True, train_accuracy=True, train_mse=True, test_mse=True, test_accuracy=True, neuron_sparsity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c79b1b",
   "metadata": {},
   "source": [
    "# 1. The Functional Approach: `scnn.optimize`\n",
    "\n",
    "The simplest way to train a neural network with convex optimization is to call `optimize` with the intended problem formulation, a training dataset, some (optional) test data, and a regularizer. \n",
    "In this case, we train a neural network with gated ReLU activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20ec14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _ = optimize(\"gated_relu\", \n",
    "                          max_neurons, \n",
    "                          X_train, \n",
    "                          y_train, \n",
    "                          X_test, \n",
    "                          y_test,\n",
    "                          regularizer=NeuronGL1(0.01),\n",
    "                          verbose=True,  \n",
    "                          device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0975a12a",
   "metadata": {},
   "source": [
    "# 2. The Object-Oriented Approach: `scnn.optimize_model`\n",
    "\n",
    "For more control over the model and optimization procedure, we can use `optimize_model`.\n",
    "We instantiate the convex formulation ourselves and choose an appropriate solver. \n",
    "In this approach, we set the gate vectors for the convex reformulation manually.\n",
    "We can also directly specify the optimizer parameters if we so choose.\n",
    "The following code trains an identical neural network as in the first approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c0a72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate convex model and other options.\n",
    "model = ConvexGatedReLU(G)\n",
    "solver = RFISTA(model, tol=1e-6)\n",
    "regularizer = NeuronGL1(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635a0f46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grelu_model, grelu_metrics = optimize_model(\n",
    "    model,\n",
    "    solver,\n",
    "    metrics,\n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_test, \n",
    "    y_test,\n",
    "    regularizer=regularizer,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e728fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Accuracy\n",
    "np.sum(np.sign(grelu_model(X_train)) == y_train) / len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317c6bd1",
   "metadata": {},
   "source": [
    "## 2.1 Changing the Optimization Method\n",
    "\n",
    "The main advantage of the second (object-oriented) approach is that it is easy to change the optimization method used.\n",
    "Previously, we trained a gated ReLU model using R-FISTA, a solver for unconstrained problems based on proximal-gradient methods.\n",
    "Now we train a variety of optimization methods leading to different final models.\n",
    "\n",
    "### 2.1.1 Cone Decompositions\n",
    "\n",
    "We want to train a ReLU model, but directly solving the corresponding convex optimization problem, which has complicating constraints, can be costly.\n",
    "In this case, we use an approximate cone decomposition to convert a gated ReLU model into a ReLU neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44296951",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvexGatedReLU(G) # start with a Gated ReLU model; a ReLU model will be output.\n",
    "solver = ApproximateConeDecomposition(model)\n",
    "cd_model, cd_metrics = optimize_model(\n",
    "    model,\n",
    "    solver,\n",
    "    metrics,\n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_test, \n",
    "    y_test,\n",
    "    regularizer,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f84680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Accuracy\n",
    "np.sum(np.sign(cd_model(X_train)) == y_train) / len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926d9a2c",
   "metadata": {},
   "source": [
    "### 2.1.2 Direct ReLU Training\n",
    "\n",
    "Of course, sometimes we prefer to directly solve the convex formulation of the ReLU training problem. \n",
    "We can use the built-in augmented Lagrangian method (AL) to do this.\n",
    "One advantage of this approach is that it produces models with smaller weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffbecf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = ConvexReLU(G)\n",
    "solver = AL(model)\n",
    "relu_model, relu_metrics = optimize_model(\n",
    "    model,\n",
    "    solver,\n",
    "    metrics,\n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_test, \n",
    "    y_test,\n",
    "    regularizer,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a122a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Accuracy\n",
    "np.sum(np.sign(relu_model(X_train)) == y_train) / len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee61c1d",
   "metadata": {},
   "source": [
    "### 2.1.3 High-Accuracy Interior Point Methods\n",
    "\n",
    "The R-FISTA and AL methods are suitable for generating moderate-accuracy solutions, fast. \n",
    "For very-high accuracy solutions, we use CVXPY as an iterface to open-source and commerical interior point methods.\n",
    "Interior point method do not produce (neuron) sparse solutions in general, so we provide a post-optimization clean-up phase that sparsifies the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754e5cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvexGatedReLU(G)\n",
    "# note that commercial solvers like MOSEK/Gurobi can be used if they are installed.\n",
    "solver = CVXPYSolver(model, \"ecos\", clean_sol=True)\n",
    "regularizer = NeuronGL1(0.01)\n",
    "cvxpy_model, cvxpy_metrics = optimize_model(\n",
    "    model,\n",
    "    solver,\n",
    "    metrics,\n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_test, \n",
    "    y_test,\n",
    "    regularizer=regularizer,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a505ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Accuracy\n",
    "np.sum(np.sign(cvxpy_model(X_train)) == y_train) / len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301036a7",
   "metadata": {},
   "source": [
    "### 2.1.4 Fast Quadratic Solvers\n",
    "\n",
    "Finally, we can use super-fast iterative solvers by changing the model formulation to make the entire problem quadratic.\n",
    "Specifically, changing the regularizer to a L2-squared penalty for gated ReLU models yields a ridge-regression problem that does not correspond to a non-convex model. \n",
    "However, it performs comparably in practice and can be trained quickly even on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d7c97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Super-fast least-squares solver.\n",
    "model = ConvexGatedReLU(G)\n",
    "solver = LeastSquaresSolver(model, tol=1e-8)\n",
    "regularizer = L2(0.01)\n",
    "lstsq_model, lstsq_metrics = optimize_model(\n",
    "    model,\n",
    "    solver,\n",
    "    metrics,\n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_test, \n",
    "    y_test,\n",
    "    regularizer=regularizer,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19a6919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Accuracy\n",
    "np.sum(np.sign(lstsq_model(X_train)) == y_train) / len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c999b2d",
   "metadata": {},
   "source": [
    "# 3. Training Times and Test Metrics\n",
    "\n",
    "We briefly summarize results for the different optimizers and models discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7327ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,6))\n",
    "spec = fig.add_gridspec(ncols=3, nrows=1)\n",
    "ax0 = fig.add_subplot(spec[0, 0])\n",
    "\n",
    "ax0.plot(np.arange(len(relu_metrics.objective)), \n",
    "         relu_metrics.objective, \n",
    "         label=\"ReLU\", \n",
    "         color=\"#ff7f0e\",\n",
    "         marker=\"^\",\n",
    "         markevery=0.1,\n",
    "         markersize=14,\n",
    "         linewidth=\"3\")\n",
    "\n",
    "ax0.plot(np.arange(len(grelu_metrics.objective)), \n",
    "         grelu_metrics.objective, \n",
    "         label=\"Gated ReLU\", \n",
    "         color=\"#1f77b4\",\n",
    "         marker=\"v\", \n",
    "         markevery=0.1,\n",
    "         markersize=14,\n",
    "         linewidth=\"3\"\n",
    "        )\n",
    "\n",
    "ax0.plot(np.arange(len(cd_metrics.objective)), \n",
    "         cd_metrics.objective, \n",
    "         label=\"Cone Decomp.\", \n",
    "         color=\"#2ca02c\",\n",
    "         marker=\"X\", \n",
    "         markevery=0.1,\n",
    "         markersize=14,\n",
    "         linewidth=\"3\"\n",
    "        )\n",
    "\n",
    "ax0.plot(np.arange(len(lstsq_metrics.objective)), \n",
    "         lstsq_metrics.objective, \n",
    "         label=\"Ridge Regression\", \n",
    "         color=\"#d62728\",\n",
    "         marker=\"X\", \n",
    "         markevery=0.1,\n",
    "         markersize=14,\n",
    "         linewidth=\"3\"\n",
    "        )\n",
    "ax0.set_yscale(\"log\")\n",
    "ax0.set_title(\"Training Objective\", fontsize=22)\n",
    "ax0.set_xlabel(\"Iterations\", fontsize=18)\n",
    "\n",
    "ax1 = fig.add_subplot(spec[0, 1])\n",
    "\n",
    "ax1.plot(relu_metrics.time, \n",
    "         relu_metrics.train_accuracy, \n",
    "         label=\"Relu\", \n",
    "         color=\"#ff7f0e\",\n",
    "         marker=\"^\",\n",
    "         markevery=0.1,\n",
    "         markersize=14,\n",
    "         linewidth=\"3\")\n",
    "\n",
    "ax1.plot(grelu_metrics.time, \n",
    "         grelu_metrics.train_accuracy, \n",
    "         label=\"Gated ReLU\", \n",
    "         color=\"#1f77b4\",\n",
    "         marker=\"v\", \n",
    "         markevery=0.1,\n",
    "         markersize=14,\n",
    "         linewidth=\"3\"\n",
    "        )\n",
    "\n",
    "ax1.plot(cd_metrics.time, \n",
    "         cd_metrics.train_accuracy, \n",
    "         label=\"Cone Decomp.\", \n",
    "         color=\"#2ca02c\",\n",
    "         marker=\"X\", \n",
    "         markevery=0.1,\n",
    "         markersize=14,\n",
    "         linewidth=\"3\"\n",
    "        )\n",
    "\n",
    "ax1.plot(lstsq_metrics.time, \n",
    "         lstsq_metrics.train_accuracy, \n",
    "         label=\"Ridge Regression\", \n",
    "         color=\"#d62728\",\n",
    "         marker=\"X\", \n",
    "         markevery=0.1,\n",
    "         markersize=14,\n",
    "         linewidth=\"3\"\n",
    "        )\n",
    "ax1.set_xscale(\"log\")\n",
    "ax1.set_ylim([0.5, 1])\n",
    "ax1.set_title(\"Training Accuracy\", fontsize=22)\n",
    "ax1.set_xlabel(\"Time (S)\", fontsize=18)\n",
    "\n",
    "ax2 = fig.add_subplot(spec[0, 2])\n",
    "\n",
    "\n",
    "ax2.plot(relu_metrics.time, \n",
    "         relu_metrics.test_accuracy, \n",
    "         label=\"Relu\", \n",
    "         color=\"#ff7f0e\",\n",
    "         marker=\"^\",\n",
    "         markevery=0.1,\n",
    "         markersize=14,\n",
    "         linewidth=\"3\")\n",
    "\n",
    "ax2.plot(grelu_metrics.time, \n",
    "         grelu_metrics.test_accuracy, \n",
    "         label=\"Gated ReLU\", \n",
    "         color=\"#1f77b4\",\n",
    "         marker=\"v\", \n",
    "         markevery=0.1,\n",
    "         markersize=14,\n",
    "         linewidth=\"3\"\n",
    "        )\n",
    "\n",
    "ax2.plot(cd_metrics.time, \n",
    "         cd_metrics.test_accuracy, \n",
    "         label=\"Cone Decomp.\", \n",
    "         color=\"#2ca02c\",\n",
    "         marker=\"X\", \n",
    "         markevery=0.1,\n",
    "         markersize=14,\n",
    "         linewidth=\"3\"\n",
    "        )\n",
    "ax2.plot(lstsq_metrics.time, \n",
    "         lstsq_metrics.test_accuracy, \n",
    "         label=\"Ridge Regression\", \n",
    "         color=\"#d62728\",\n",
    "         marker=\"X\", \n",
    "         markevery=0.1,\n",
    "         markersize=14,\n",
    "         linewidth=\"3\"\n",
    "        )\n",
    "ax2.set_xscale(\"log\")\n",
    "ax2.set_ylim([0.5, 1])\n",
    "ax2.set_title(\"Test Accuracy\", fontsize=22)\n",
    "ax2.set_xlabel(\"Time (S)\", fontsize=18)\n",
    "handles, labels = ax0.get_legend_handles_labels()\n",
    "legend = fig.legend(\n",
    "    handles=handles,\n",
    "    labels=labels,\n",
    "    loc=\"lower center\",\n",
    "    borderaxespad=0.1,\n",
    "    fancybox=False,\n",
    "    shadow=False,\n",
    "    ncol=4,\n",
    "    fontsize=16,\n",
    "    frameon=False,\n",
    ")\n",
    "fig.subplots_adjust(\n",
    "    bottom=0.15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acfb696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
